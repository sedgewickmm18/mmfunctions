{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: confluent_kafka is not installed. Publish to MessageHub not supported.\n",
      "/home/markus/.local/lib/python3.8/site-packages/iotfunctions/bif.py:1877: UserWarning: IoTCalcSettings is deprecated. Use entity type constants instead of a metadata provider to set entity type properties\n",
      "  warnings.warn(('IoTCalcSettings is deprecated. Use entity type constants'\n"
     ]
    }
   ],
   "source": [
    "# Real life data\n",
    "\n",
    "import logging\n",
    "import threading\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as seabornInstance\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, func\n",
    "from iotfunctions import base\n",
    "from iotfunctions import bif\n",
    "from iotfunctions import entity\n",
    "from iotfunctions import metadata\n",
    "from iotfunctions.metadata import EntityType\n",
    "from iotfunctions.db import Database\n",
    "from iotfunctions.enginelog import EngineLogging\n",
    "from iotfunctions import estimator\n",
    "from iotfunctions.ui import (UISingle, UIMultiItem, UIFunctionOutSingle,\n",
    "                 UISingleItem, UIFunctionOutMulti, UIMulti, UIExpression,\n",
    "                 UIText, UIStatusFlag, UIParameters)\n",
    "from mmfunctions.anomaly import (SaliencybasedGeneralizedAnomalyScore, SpectralAnomalyScore,\n",
    "                 FFTbasedGeneralizedAnomalyScore, KMeansAnomalyScore)\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, r2_score\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.fftpack\n",
    "import skimage as ski\n",
    "\n",
    "from skimage import util as skiutil # for nifty windowing\n",
    "import pyod as pyod\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "%matplotlib inline\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "EngineLogging.configure_console_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a 2-layered LSTM in Watson Machine Learning\n",
    "\n",
    " \n",
    "Telemanom ([Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding](https://arxiv.org/pdf/1802.04431.pdf) - 2018\n",
    "\n",
    "\n",
    "Let's find out first what ML libraries are supported by WML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------  --------------------------  ------------------------  --------\n",
      "GUID                        NAME                        CREATED                   PLATFORM\n",
      "do_12.10                    do_12.10                    2020-03-20T04:19:17.471Z  do\n",
      "xgboost_0.90-py3.6          xgboost_0.90-py3.6          2020-03-20T04:19:03.205Z  python\n",
      "scikit-learn_0.22-py3.6     scikit-learn_0.22-py3.6     2020-03-20T04:18:53.589Z  python\n",
      "spark-mllib_2.4             spark-mllib_2.4             2020-02-06T09:30:35.538Z  spark\n",
      "tensorflow_1.15-py3.6       tensorflow_1.15-py3.6       2020-02-06T09:30:30.574Z  python\n",
      "pytorch-onnx_1.2-py3.6      pytorch-onnx_1.2-py3.6      2020-02-06T09:29:58.456Z  python\n",
      "pytorch-onnx_1.2-py3.6-edt  pytorch-onnx_1.2-py3.6-edt  2020-02-06T09:29:54.031Z  python\n",
      "tensorflow_1.14-py3.6       tensorflow_1.14-py3.6       2019-10-23T09:54:32.847Z  python\n",
      "pytorch-onnx_1.1-py3.6      pytorch-onnx_1.1-py3.6      2019-10-23T09:54:02.251Z  python\n",
      "pytorch-onnx_1.1-py3.6-edt  pytorch-onnx_1.1-py3.6-edt  2019-10-23T09:53:58.775Z  python\n",
      "pytorch_1.1-py3.6           pytorch_1.1-py3.6           2019-10-23T09:53:56.218Z  python\n",
      "pytorch_1.1-py3             pytorch_1.1-py3             2019-10-23T09:53:53.712Z  python\n",
      "xgboost_0.82-py3.6          xgboost_0.82-py3.6          2019-08-02T06:02:55.531Z  python\n",
      "xgboost_0.80-py3.6          xgboost_0.80-py3.6          2019-08-02T06:02:52.157Z  python\n",
      "scikit-learn_0.20-py3.6     scikit-learn_0.20-py3.6     2019-08-02T06:02:45.108Z  python\n",
      "scikit-learn_0.19-py3.6     scikit-learn_0.19-py3.6     2019-08-02T06:02:41.790Z  python\n",
      "tensorflow_1.13-py3.6       tensorflow_1.13-py3.6       2019-08-02T06:02:34.144Z  python\n",
      "tensorflow_1.11-py3.6       tensorflow_1.11-py3.6       2019-08-02T06:02:31.678Z  python\n",
      "tensorflow_1.5-py3.6        tensorflow_1.5-py3.6        2019-08-02T06:02:12.855Z  python\n",
      "ai-function_0.1-py3.6       ai-function_0.1-py3.6       2019-07-05T12:02:38.754Z  python\n",
      "xgboost_0.82-py3            xgboost_0.82-py3            2019-07-05T12:02:33.447Z  python\n",
      "spss-modeler_18.2           spss-modeler_18.2           2019-07-05T12:02:29.092Z  spss\n",
      "scikit-learn_0.20-py3       scikit-learn_0.20-py3       2019-07-05T12:02:24.736Z  python\n",
      "pmml_4.3                    pmml_4.3                    2019-04-15T09:59:34.202Z  pmml\n",
      "pmml_4.2                    pmml_4.2                    2019-04-15T09:59:30.855Z  pmml\n",
      "pmml_4.1                    pmml_4.1                    2019-04-15T09:59:28.401Z  pmml\n",
      "pmml_4.0                    pmml_4.0                    2019-04-15T09:59:25.948Z  pmml\n",
      "pmml_3.2                    pmml_3.2                    2019-04-15T09:59:23.480Z  pmml\n",
      "pmml_3.1                    pmml_3.1                    2019-04-15T09:59:20.942Z  pmml\n",
      "pmml_3.0                    pmml_3.0                    2019-04-15T09:59:18.466Z  pmml\n",
      "do_12.9                     do_12.9                     2019-04-04T10:55:24.375Z  do\n",
      "pmml_4.2.1                  pmml_4.2.1                  2019-04-04T10:55:21.561Z  pmml\n",
      "ai-function_0.1-py3         ai-function_0.1-py3         2019-04-04T10:55:18.663Z  python\n",
      "hybrid_0.2                  hybrid_0.2                  2019-04-04T10:55:15.872Z  hybrid\n",
      "hybrid_0.1                  hybrid_0.1                  2019-04-04T10:55:13.073Z  hybrid\n",
      "xgboost_0.80-py3            xgboost_0.80-py3            2019-04-04T10:55:10.280Z  python\n",
      "xgboost_0.6-py3             xgboost_0.6-py3             2019-04-04T10:55:07.493Z  python\n",
      "spss-modeler_18.1           spss-modeler_18.1           2019-04-04T10:55:04.706Z  spss\n",
      "spss-modeler_17.1           spss-modeler_17.1           2019-04-04T10:55:01.919Z  spss\n",
      "scikit-learn_0.19-py3       scikit-learn_0.19-py3       2019-04-04T10:54:59.060Z  python\n",
      "scikit-learn_0.17-py3       scikit-learn_0.17-py3       2019-04-04T10:54:56.269Z  python\n",
      "spark-mllib_2.3             spark-mllib_2.3             2019-04-04T10:54:53.422Z  spark\n",
      "spark-mllib_2.2             spark-mllib_2.2             2019-04-04T10:54:50.637Z  spark\n",
      "spark-mllib_2.1             spark-mllib_2.1             2019-04-04T10:54:47.851Z  spark\n",
      "tensorflow_1.13-py3         tensorflow_1.13-py3         2019-04-04T10:54:45.039Z  python\n",
      "tensorflow_1.13-py2         tensorflow_1.13-py2         2019-04-04T10:54:42.256Z  python\n",
      "tensorflow_0.11-horovod     tensorflow_0.11-horovod     2019-04-04T10:54:39.420Z  native\n",
      "tensorflow_1.11-py3         tensorflow_1.11-py3         2019-04-04T10:54:36.622Z  python\n",
      "tensorflow_1.10-py3         tensorflow_1.10-py3         2019-04-04T10:54:33.741Z  python\n",
      "tensorflow_1.10-py2         tensorflow_1.10-py2         2019-04-04T10:54:30.915Z  python\n",
      "tensorflow_1.9-py3          tensorflow_1.9-py3          2019-04-04T10:54:27.968Z  python\n",
      "tensorflow_1.9-py2          tensorflow_1.9-py2          2019-04-04T10:54:25.170Z  python\n",
      "tensorflow_1.8-py3          tensorflow_1.8-py3          2019-04-04T10:54:22.330Z  python\n",
      "tensorflow_1.8-py2          tensorflow_1.8-py2          2019-04-04T10:54:19.534Z  python\n",
      "tensorflow_1.7-py3          tensorflow_1.7-py3          2019-04-04T10:54:16.741Z  python\n",
      "tensorflow_1.7-py2          tensorflow_1.7-py2          2019-04-04T10:54:13.963Z  python\n",
      "tensorflow_1.6-py3          tensorflow_1.6-py3          2019-04-04T10:54:11.163Z  python\n",
      "tensorflow_1.6-py2          tensorflow_1.6-py2          2019-04-04T10:54:08.371Z  python\n",
      "tensorflow_1.5-py2-ddl      tensorflow_1.5-py2-ddl      2019-04-04T10:54:05.592Z  python\n",
      "tensorflow_1.5-py3-horovod  tensorflow_1.5-py3-horovod  2019-04-04T10:54:02.749Z  python\n",
      "tensorflow_1.5-py3          tensorflow_1.5-py3          2019-04-04T10:53:59.968Z  python\n",
      "tensorflow_1.5-py2          tensorflow_1.5-py2          2019-04-04T10:53:57.099Z  python\n",
      "tensorflow_1.4-py2-ddl      tensorflow_1.4-py2-ddl      2019-04-04T10:53:54.318Z  python\n",
      "tensorflow_1.4-py3-horovod  tensorflow_1.4-py3-horovod  2019-04-04T10:53:51.540Z  python\n",
      "tensorflow_1.4-py3          tensorflow_1.4-py3          2019-04-04T10:53:48.740Z  python\n",
      "tensorflow_1.4-py2          tensorflow_1.4-py2          2019-04-04T10:53:45.962Z  python\n",
      "tensorflow_1.3-py2-ddl      tensorflow_1.3-py2-ddl      2019-04-04T10:53:43.176Z  python\n",
      "tensorflow_1.3-py3          tensorflow_1.3-py3          2019-04-04T10:53:40.401Z  python\n",
      "tensorflow_1.3-py2          tensorflow_1.3-py2          2019-04-04T10:53:37.610Z  python\n",
      "tensorflow_1.2-py3          tensorflow_1.2-py3          2019-04-04T10:53:34.841Z  python\n",
      "tensorflow_1.2-py2          tensorflow_1.2-py2          2019-04-04T10:53:32.032Z  python\n",
      "pytorch-onnx_1.0-py3        pytorch-onnx_1.0-py3        2019-04-04T10:53:29.246Z  python\n",
      "pytorch_1.0-py3-edt         pytorch_1.0-py3-edt         2019-04-04T10:53:26.273Z  python\n",
      "pytorch_1.0-py2-edt         pytorch_1.0-py2-edt         2019-04-04T10:53:23.495Z  python\n",
      "pytorch_1.0-py3             pytorch_1.0-py3             2019-04-04T10:53:20.707Z  python\n",
      "pytorch_1.0-py2             pytorch_1.0-py2             2019-04-04T10:53:17.933Z  python\n",
      "pytorch_0.4-py3-horovod     pytorch_0.4-py3-horovod     2019-04-04T10:53:15.163Z  python\n",
      "pytorch_0.4-py3             pytorch_0.4-py3             2019-04-04T10:53:12.392Z  python\n",
      "pytorch_0.4-py2             pytorch_0.4-py2             2019-04-04T10:53:09.623Z  python\n",
      "pytorch_0.3-py3             pytorch_0.3-py3             2019-04-04T10:53:06.785Z  python\n",
      "pytorch_0.3-py2             pytorch_0.3-py2             2019-04-04T10:53:04.005Z  python\n",
      "torch_lua52                 torch_lua52                 2019-04-04T10:53:01.226Z  lua\n",
      "torch_luajit                torch_luajit                2019-04-04T10:52:58.465Z  lua\n",
      "caffe-ibm_1.0-py3           caffe-ibm_1.0-py3           2019-04-04T10:52:55.622Z  python\n",
      "caffe-ibm_1.0-py2           caffe-ibm_1.0-py2           2019-04-04T10:52:52.862Z  python\n",
      "caffe_1.0-py3               caffe_1.0-py3               2019-04-04T10:52:50.101Z  python\n",
      "caffe_1.0-py2               caffe_1.0-py2               2019-04-04T10:52:47.261Z  python\n",
      "caffe_frcnn                 caffe_frcnn                 2019-04-04T10:52:44.492Z  Python\n",
      "caffe_1.0-ddl               caffe_1.0-ddl               2019-04-04T10:52:41.727Z  native\n",
      "caffe2_0.8                  caffe2_0.8                  2019-04-04T10:52:38.971Z  Python\n",
      "darknet_0                   darknet_0                   2019-04-04T10:52:36.028Z  native\n",
      "theano_1.0                  theano_1.0                  2019-04-04T10:52:33.192Z  Python\n",
      "mxnet_1.2-py2               mxnet_1.2-py2               2019-04-04T10:52:30.427Z  python\n",
      "mxnet_1.1-py2               mxnet_1.1-py2               2019-04-04T10:52:27.598Z  python\n",
      "--------------------------  --------------------------  ------------------------  --------\n"
     ]
    }
   ],
   "source": [
    "# make sure to downgrade to sklearn 0.22.2 (no >= 0.23)\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "with open('credentials_wml.json', encoding='utf-8') as F:\n",
    "    wml_credentials = json.loads(F.read())\n",
    "    \n",
    "with open('credentials_cos.json', encoding='utf-8') as F:\n",
    "    cos_credentials = json.loads(F.read())\n",
    "\n",
    "wml_url=wml_credentials['url']\n",
    "wml_instance_id=wml_credentials['instance_id']\n",
    "wml_apikey=wml_credentials['apikey']\n",
    "\n",
    "wml_data_source_type= 's3'\n",
    "\n",
    "\n",
    "# don't use this endpoint\n",
    "cos_endpoint = cos_credentials['endpoints']\n",
    "cos_endpoint = 'https://s3.eu.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "cos_apikey = cos_credentials['apikey']\n",
    "cos_access_key = cos_credentials['cos_hmac_keys']['access_key_id']\n",
    "cos_secret_key = cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "# 'https://s3.eu.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "cos_input_bucket = 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in'\n",
    "cos_output_bucket = 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in'\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "rep_list = client.runtimes.list(limit=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step \n",
    "\n",
    "Apparently Keras is missing out. However, Telemanom is built on Keras, so we have to port it to either Tensorflow, Pytorch, Mxnet, Caffe or Theano.\n",
    "\n",
    "I opted for Pytorch for skill building purposes and ported Telemanom to Pytorch.\n",
    "\n",
    "\n",
    "\n",
    "<small>\n",
    "    \n",
    "```\n",
    "    \n",
    "class LSTM_2L(nn.Module):\n",
    "    def __init__(self, n_features = 1, hidden_dims = [80,80], seq_length = 250,\n",
    "                 batch_size = 64, n_predictions = 10, dropout = 0.3):\n",
    "        super(LSTM_2L, self).__init__()\n",
    "        print ('LSTM_2L', n_features, hidden_dims, seq_length, batch_size, n_predictions, dropout)\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = len(self.hidden_dims)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size = self.n_features,\n",
    "            hidden_size = self.hidden_dims[0],\n",
    "            batch_first = True,\n",
    "            dropout = dropout,\n",
    "            num_layers = 2)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dims[1], n_predictions)\n",
    "        self.init_hidden_state()\n",
    "        \n",
    "    def init_hidden_state(self):\n",
    "\n",
    "        self.hidden = (\n",
    "            torch.randn(self.num_layers, self.batch_size, self.hidden_dims[0]), #.to(self.device),\n",
    "            torch.randn(self.num_layers, self.batch_size, self.hidden_dims[0]), #.to(self.device),\n",
    "            )\n",
    "\n",
    "    def forward(self, sequences):\n",
    "\n",
    "        batch_size, seq_len, n_features = sequences.size()  # batch first\n",
    "\n",
    "        lstm1_out , (h1_n, c1_n) = self.lstm1(sequences, (self.hidden[0], self.hidden[1]))\n",
    "\n",
    "        last_time_step = lstm1_out[:,-1,:]\n",
    "\n",
    "        y_pred = self.linear(last_time_step)\n",
    "\n",
    "        return y_pred\n",
    " ```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# part of mmfunctions\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import telemanom\n",
    "from telemanom.helpers import Config\n",
    "from telemanom.errors import Errors\n",
    "import telemanom.helpers as helpers\n",
    "from telemanom.channel import Channel\n",
    "from telemanom.modeling import Model\n",
    "\n",
    "conf = Config(\"./telemanom/config.yaml\")\n",
    "\n",
    "conf.dictionary['l_s'] = 250\n",
    "conf.dictionary['epochs'] = 80\n",
    "conf.dictionary['dropout'] = 0.2\n",
    "conf.batch_size = 512\n",
    "conf.l_s = 250\n",
    "conf.epochs = 80    # max\n",
    "conf.dropout = 0.2\n",
    "conf.lstm_batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Channel:Channel\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Define structure for local data\n",
    "#              telemanom supports multiple channels to reflect spacecraft sensors, we only need a single one now\n",
    "#\n",
    "device=\"Armstarknew\"\n",
    "chan = Channel(conf, device)\n",
    "helpers.make_dirs(conf.use_id, conf, \"./telemanom\")\n",
    "print(chan)\n",
    "conf\n",
    "\n",
    "# load data\n",
    "\n",
    "chan.train = np.loadtxt('./telemanom/wml_train.csv')\n",
    "chan.test = np.loadtxt('./telemanom/wml_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following steps replay the code in wml_telemanom.py\n",
    "\n",
    "We jump over the next few cells unless we want to initiate a local training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-27T18:17:48.553 INFO telemanom.shape_data FFT channel: False\n",
      "(129300, 2)\n",
      "2020-07-27T18:17:49.010 INFO telemanom.shape_data FFT channel: False\n",
      "(129195, 2)\n"
     ]
    }
   ],
   "source": [
    "# producing overlapping windows of length 260 for lookback (250) and prediction (10)\n",
    "chan.shape_data(chan.train, train=True)\n",
    "chan.shape_data(chan.test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2L 2 [80, 80] 250 64 10 0.2\n",
      "init hidden state\n",
      "Hidden dimension 0:  2 64 80\n",
      "Hidden dimension 1:  2 64 80\n",
      "input shape:  (None, 2)\n"
     ]
    }
   ],
   "source": [
    "# init the Python double stacked LSTM model\n",
    "model = Model(conf, conf.use_id, chan, \"./telemanom\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    model.model.load_state_dict(torch.load(\"./mytrainedpytorchmodel\"))\n",
    "    model.model.eval()\n",
    "except Exception:\n",
    "    # drink a coffee - training takes roughly 30 minutes\n",
    "    model.train_new(chan)\n",
    "    torch.save(model.model.state_dict(), \"./mytrainedpytorchmodel\")\n",
    "\n",
    "# no training run - we've already spent CPU cycles last week\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of LSTM_2L(\n",
       "  (lstm1): LSTM(2, 80, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (linear): Linear(in_features=80, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8453,  2.7198, -1.7876,  ..., -0.5158, -0.4389, -0.8363],\n",
       "         [-0.4767,  2.9927, -0.0612,  ...,  0.2423,  1.2901,  0.9106],\n",
       "         [-1.0040,  0.4957, -0.7384,  ..., -0.6494,  0.0917,  1.4230],\n",
       "         ...,\n",
       "         [-0.3324,  0.3252,  1.8514,  ...,  0.6480, -0.5821,  0.0355],\n",
       "         [ 0.5027, -1.8701,  0.6032,  ..., -0.1787, -0.2139,  0.7462],\n",
       "         [ 0.3347,  0.9087,  2.4258,  ..., -0.1098,  1.7664, -1.8507]],\n",
       "\n",
       "        [[-0.2791,  0.3428,  0.1063,  ...,  0.4518,  1.3893,  0.3305],\n",
       "         [ 0.3864, -0.4589,  1.1742,  ..., -0.5003,  0.1187,  0.0829],\n",
       "         [-0.1722, -0.2796, -0.1617,  ...,  1.9113,  0.2454,  0.9651],\n",
       "         ...,\n",
       "         [ 0.3260, -2.8935,  0.8442,  ...,  0.3674, -0.3341,  1.7589],\n",
       "         [-1.0542, -0.3836,  2.1828,  ...,  1.0550,  0.8760, -0.3506],\n",
       "         [-0.0093,  0.5758,  0.4889,  ...,  0.0326,  0.4176, -0.0619]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.hidden[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward:  64 80 2\n",
      "forward:  tensor(64) tensor(80) tensor(2)\n",
      "name: \"input.1\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_value: 64\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 80\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 2\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# attempt to export it as ONNX model\n",
    "\n",
    "# switch off training mode\n",
    "model.model.eval()\n",
    "\n",
    "# switch off autograd, automatic differentiation\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # input tensor\n",
    "    x = torch.randn(64, 80, 2, requires_grad=True)\n",
    "    \n",
    "    # test dimensions\n",
    "    output = model.model(x).tolist()\n",
    "\n",
    "    # default export\n",
    "    torch.onnx.export(model.model, x, 'lstm.onnx')\n",
    "    \n",
    "    # test model load\n",
    "    import onnx\n",
    "    onnx_model = onnx.load('lstm.onnx')\n",
    "    # input shape [5, 3, 10]\n",
    "    print(onnx_model.graph.input[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<strong>Make sure you have uploaded the code in mmfunctions/telemanom as zip file to COS bucket</strong><br/>githubanalyzer-donotdelete-pr-b9xa3kxotzh5in"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "Markdown('<strong>{}</strong><br/>{}'.format('Make sure you have uploaded the code in mmfunctions/telemanom as zip file to COS bucket', cos_input_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<strong>./telemanom/wml_model.zip\n",
       "found - good</strong><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip the code in the ./telemanom subdirectory first\n",
    "\n",
    "import subprocess\n",
    "output = None\n",
    "try:\n",
    "    output = subprocess.check_output(\"ls ./telemanom/wml_model.zip\", shell=True).decode('ascii')  + 'found - good'\n",
    "except Exception:\n",
    "    output = 'Not found - do it now and run \\\"zip -x \\'.git*\\' -9ry wml_model.zip  .\\\" in the telemanom directory'\n",
    "\n",
    "Markdown('<strong>{}</strong><br/>'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/_wml_checkpoints/\r\n",
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/bf25a013-c8e9-4501-a8af-bd5a3bbc22a6/\r\n",
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/notebook/\r\n",
      "                          DIR  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/training-h9VOfZVMR/\r\n",
      "2020-07-27 14:35      2678613  s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/wml_model.zip\r\n"
     ]
    }
   ],
   "source": [
    "# check whether we have uploaded the code\n",
    "!s3cmd --access_key {cos_access_key} --secret_key {cos_secret_key} \\\n",
    "--access_token {cos_apikey} --host s3.eu.cloud-object-storage.appdomain.cloud --host-bucket=s3.eu.cloud-object-storage.appdomain.cloud \\\n",
    "ls s3://githubanalyzer-donotdelete-pr-b9xa3kxotzh5in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now starting to work with WML\n",
    "#\n",
    "#   make sure we go with Open Neural Network Exchange (ONNX) to allow for pytorch model exporting\n",
    "# \n",
    "\n",
    "wml_train_code='./telemanom/wml_model.zip' # where this notebook finds the code\n",
    "\n",
    "wml_execution_command='python3 wml_telemanom.py' # command to start training\n",
    "\n",
    "wml_framework_name='pytorch-onnx'\n",
    "wml_framework_version='1.1' # we run on pytorch-onnx 1.2 (Open Neural Network Exchange)\n",
    "wml_runtime = 'python'\n",
    "wml_runtime_version='3.6' # and python 3.6\n",
    "\n",
    "wml_run_definition = 'wml-pytorch-definition' # dummy name\n",
    "wml_run_name = 'wml-pytorch-run' # more dummy\n",
    "wml_model_name='wml-tensorflow-miregal' # even more dummy\n",
    "\n",
    "wml_compute_name='k80'  # free tier machine type\n",
    "wml_compute_nodes='1'   # free tier\n",
    "\n",
    "wml_runtime_version_v4 = wml_framework_version + '-py' + wml_runtime_version   # sdk level\n",
    "wml_compute_nodes_v4 = int(wml_compute_nodes)\n",
    "\n",
    "model_code = wml_train_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./telemanom/wml_model.zip'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wml_train_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   (custom) libraries serve as meta data for model code to be installed on top of predefined container images\n",
    "#\n",
    "# define library meta data for our training code\n",
    "#\n",
    "lib_meta = {\n",
    "    client.runtimes.LibraryMetaNames.NAME: wml_run_definition,\n",
    "    client.runtimes.LibraryMetaNames.VERSION: wml_framework_version,\n",
    "    client.runtimes.LibraryMetaNames.FILEPATH: model_code,\n",
    "    client.runtimes.LibraryMetaNames.PLATFORM: {\"name\": wml_framework_name, \"versions\": [wml_framework_version]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete  {'metadata': {'guid': 'f7ec0ace-2dd6-4193-9ec5-b7b2bbb773b6', 'id': 'f7ec0ace-2dd6-4193-9ec5-b7b2bbb773b6', 'modified_at': '2020-07-27T16:59:52.845Z', 'created_at': '2020-07-27T16:59:39.819Z', 'href': '/v4/libraries/f7ec0ace-2dd6-4193-9ec5-b7b2bbb773b6'}, 'entity': {'space': {'id': '88740b60-6b2f-4f74-b6d8-20528d14db8b', 'href': '/v4/spaces/88740b60-6b2f-4f74-b6d8-20528d14db8b'}, 'name': 'wml-pytorch-definition', 'version': '1.2', 'platform': {'name': 'pytorch-onnx', 'versions': ['1.2']}}}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# do we have a library with that name defined ?\n",
    "#   delete it first and then store the new updated library\n",
    "#\n",
    "library_details = client.runtimes.get_library_details()\n",
    "for library_detail in library_details['resources']:\n",
    "    if library_detail['entity']['name'] == wml_run_definition:\n",
    "        # Delete library if exist because we cannot update model_code\n",
    "        uid = client.runtimes.get_library_uid(library_detail)\n",
    "        print ('delete ', library_detail)\n",
    "        client.repository.delete(uid)\n",
    "        break\n",
    "\n",
    "custom_library_details = client.runtimes.store_library(lib_meta)\n",
    "custom_library_uid = client.runtimes.get_library_uid(custom_library_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Pipelines define a sequence of operations\n",
    "#\n",
    "# define a pipeline with a single entry (node) for the training run\n",
    "#  we could add more node for scaling/normalizing, imputation, feature extraction, \"you name it\"\n",
    "#\n",
    "doc = {\n",
    "    \"doc_type\": \"pipeline\",\n",
    "    \"version\": \"2.0\",\n",
    "    \"primary_pipeline\": wml_framework_name,\n",
    "    \"pipelines\": [{\n",
    "        \"id\": wml_framework_name,\n",
    "        \"runtime_ref\": \"hybrid\",\n",
    "        \"nodes\": [{\n",
    "            \"id\": \"training\",\n",
    "            \"type\": \"model_node\",\n",
    "            \"op\": \"dl_train\",\n",
    "            \"runtime_ref\": wml_run_name,\n",
    "            \"inputs\": [],\n",
    "            \"outputs\": [],\n",
    "            \"parameters\": {\n",
    "                \"name\": \"pytorch-telemanom\",\n",
    "                \"description\": wml_run_definition,\n",
    "                \"command\": wml_execution_command,\n",
    "                \"training_lib_href\": \"/v4/libraries/\"+custom_library_uid,\n",
    "                \"compute\": {\n",
    "                    \"name\": wml_compute_name,            # specify where to run it (not that I have a choice)\n",
    "                    \"nodes\": wml_compute_nodes_v4\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    }],\n",
    "    \"runtimes\": [{\n",
    "        \"id\": wml_run_name,\n",
    "        \"name\": wml_framework_name,         # run it on a pytorch image\n",
    "        \"version\": wml_runtime_version_v4\n",
    "    }]\n",
    "}\n",
    "\n",
    "# put it in metadata object\n",
    "metadata = {\n",
    "    client.repository.PipelineMetaNames.NAME: wml_run_name,\n",
    "    client.repository.PipelineMetaNames.DOCUMENT: doc\n",
    "}\n",
    "\n",
    "# and create the pipeline\n",
    "pipeline_id = client.pipelines.get_uid(client.repository.store_pipeline(meta_props=metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'name': 'wml-pytorch-run',\n",
       "  'guid': 'e6f3b3a6-8164-437b-884f-15f63819e619',\n",
       "  'rev': 'a0f08819-65c7-4395-b5c6-b260b61d6dca',\n",
       "  'id': 'e6f3b3a6-8164-437b-884f-15f63819e619',\n",
       "  'modified_at': '2020-07-27T17:04:17.503Z',\n",
       "  'created_at': '2020-07-27T17:04:17.432Z',\n",
       "  'href': '/v4/pipelines/e6f3b3a6-8164-437b-884f-15f63819e619?rev=a0f08819-65c7-4395-b5c6-b260b61d6dca'},\n",
       " 'entity': {'space': {'id': '88740b60-6b2f-4f74-b6d8-20528d14db8b',\n",
       "   'href': '/v4/spaces/88740b60-6b2f-4f74-b6d8-20528d14db8b'},\n",
       "  'name': 'wml-pytorch-run',\n",
       "  'document': {'doc_type': 'pipeline',\n",
       "   'version': '2.0',\n",
       "   'pipelines': [{'id': 'pytorch-onnx',\n",
       "     'runtime_ref': 'hybrid',\n",
       "     'nodes': [{'outputs': [],\n",
       "       'id': 'training',\n",
       "       'inputs': [],\n",
       "       'type': 'model_node',\n",
       "       'parameters': {'name': 'pytorch-telemanom',\n",
       "        'description': 'wml-pytorch-definition',\n",
       "        'compute': {'name': 'k80', 'nodes': 1},\n",
       "        'command': 'python3 wml_telemanom.py',\n",
       "        'training_lib_href': '/v4/libraries/34c09d5b-84ee-4953-b995-a1e3a4c21bfb'},\n",
       "       'runtime_ref': 'wml-pytorch-run',\n",
       "       'op': 'dl_train'}]}],\n",
       "   'runtimes': [{'id': 'wml-pytorch-run',\n",
       "     'name': 'pytorch-onnx',\n",
       "     'version': '1.1-py3.6'}],\n",
       "   'primary_pipeline': 'pytorch-onnx'}}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is my pipeline now\n",
    "client.pipelines.get_details(pipeline_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_id {'metadata': {'created_at': '2020-07-27T17:04:18.055Z', 'guid': '26c741ed-d92e-4e49-96de-fba52fdc2a66', 'href': '/v4/trainings/26c741ed-d92e-4e49-96de-fba52fdc2a66', 'id': '26c741ed-d92e-4e49-96de-fba52fdc2a66', 'space_id': '88740b60-6b2f-4f74-b6d8-20528d14db8b'}, 'entity': {'pipeline': {'href': '/v4/pipelines/e6f3b3a6-8164-437b-884f-15f63819e619', 'id': 'e6f3b3a6-8164-437b-884f-15f63819e619'}, 'results_reference': {'location': {'training': '26c741ed-d92e-4e49-96de-fba52fdc2a66', 'pipeline_model': '26c741ed-d92e-4e49-96de-fba52fdc2a66/pipeline-model.json', 'training_status': '26c741ed-d92e-4e49-96de-fba52fdc2a66/training-status.json', 'pipeline': '26c741ed-d92e-4e49-96de-fba52fdc2a66/pipeline.json', 'bucket': 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in', 'assets_path': '26c741ed-d92e-4e49-96de-fba52fdc2a66/assets'}, 'type': 's3', 'connection': {'access_key_id': 'cc04444c99374c9e9589b8f85e931323', 'secret_access_key': '1a5062d937b09507a05b521a41b8baf6848c0cd6936e2864', 'endpoint_url': 'https://s3.eu.cloud-object-storage.appdomain.cloud'}}, 'space': {'href': '/v4/spaces/88740b60-6b2f-4f74-b6d8-20528d14db8b', 'id': '88740b60-6b2f-4f74-b6d8-20528d14db8b'}, 'space_id': '88740b60-6b2f-4f74-b6d8-20528d14db8b', 'status': {'state': 'pending'}, 'training_data_references': [{'location': {'bucket': 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in'}, 'type': 's3', 'connection': {'access_key_id': 'cc04444c99374c9e9589b8f85e931323', 'secret_access_key': '1a5062d937b09507a05b521a41b8baf6848c0cd6936e2864', 'endpoint_url': 'https://s3.eu.cloud-object-storage.appdomain.cloud'}}]}}\n",
      "get status {'state': 'pending'}\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# finally start the training run for v4\n",
    "#   tell it where to load data and model code from and dump results to\n",
    "#\n",
    "metadata = {\n",
    "    client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n",
    "        \"name\": \"training-results-reference_name\",\n",
    "        \"connection\": {\n",
    "            \"endpoint_url\": cos_endpoint,\n",
    "            \"access_key_id\": cos_access_key,\n",
    "            \"secret_access_key\": cos_secret_key\n",
    "        },\n",
    "        \"location\": {\n",
    "            \"bucket\": cos_output_bucket\n",
    "        },\n",
    "        \"type\": wml_data_source_type\n",
    "    },\n",
    "    client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCES:[{\n",
    "        \"name\": \"training_input_data\",\n",
    "        \"type\": wml_data_source_type,\n",
    "        \"connection\": {\n",
    "            \"endpoint_url\": cos_endpoint,\n",
    "            \"access_key_id\": cos_access_key,\n",
    "            \"secret_access_key\": cos_secret_key\n",
    "        },\n",
    "        \"location\": {\n",
    "            \"bucket\": cos_input_bucket\n",
    "        }\n",
    "    }],\n",
    "    client.training.ConfigurationMetaNames.PIPELINE_UID: pipeline_id\n",
    "}\n",
    "\n",
    "training_id = client.training.get_uid(client.training.run(meta_props=metadata))\n",
    "print(\"training_id\", client.training.get_details(training_id))\n",
    "print(\"get status\", client.training.get_status(training_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Log monitor started for training run: 26c741ed-d92e-4e49-96de-fba52fdc2a66\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------\n",
      "Log monitor done.\n",
      "-----------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#############################################################################\n",
      "\n",
      "Metric monitor started for training run: 26c741ed-d92e-4e49-96de-fba52fdc2a66\n",
      "\n",
      "#############################################################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "Metric monitor done.\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_details = client.training.get_details(training_id)\n",
    "run_uid = training_id\n",
    "\n",
    "# print logs\n",
    "\n",
    "client.training.monitor_logs(run_uid)\n",
    "client.training.monitor_metrics(run_uid)\n",
    "\n",
    "# should not have run after restarting the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': {'text': 'Node training: Batch  753\\n', 'level': 'info'},\n",
       " 'running_at': '2020-07-27T17:05:35.955Z',\n",
       " 'state': 'running'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run_uid='3dd32ad4-d6bf-45be-b09f-cf2b79e75e7b'\n",
    "status = client.training.get_status(run_uid)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if training seems to have failed, just look at the logs in our COS output bucket\n",
    "\n",
    "Dumb me, in the previous run I forgot to import sys.\n",
    "\n",
    "Fortunately model training has succeeded and the model has been stored in COS. Phew.\n",
    "\n",
    "<small>\n",
    "\n",
    "```\n",
    "...\n",
    "Batch  1611\n",
    "Batch  1612\n",
    "After batch  1612 0.002384878075476655\n",
    "[1] Training loss: 0.002384878075476655 \t Validation loss: 0.0014487287297119242 \n",
    "Training complete...\n",
    "Model saved in file: /mnt/results/githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/training-h9VOfZVMR/model\n",
    "['_submitted_code', 'learner-1', 'model', 'training-log.txt']\n",
    "/mnt/results/githubanalyzer-donotdelete-pr-b9xa3kxotzh5in/training-h9VOfZVMR\n",
    "Traceback (most recent call last):\n",
    "  File \"wml_telemanom.py\", line 61, in <module>\n",
    "    sys.stdout.flush()\n",
    "NameError: name 'sys' is not defined\n",
    "```\n",
    "    \n",
    "</small>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1-py3.6'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wml_runtime_version_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store the model\n",
    "\n",
    "meta_props_pyt = {\n",
    "    client.repository.ModelMetaNames.NAME: wml_model_name,\n",
    "    client.repository.ModelMetaNames.RUNTIME_UID: wml_framework_name + '_' + wml_runtime_version_v4,\n",
    "    client.repository.ModelMetaNames.TYPE: wml_framework_name + '_' + wml_framework_version\n",
    "}\n",
    "\n",
    "model_details = client.repository.store_model(run_uid, meta_props=meta_props_pyt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'name': 'wml-tensorflow-miregal',\n",
       "  'guid': 'eeac7d0a-69e7-4f81-ac5b-35eff542a841',\n",
       "  'rev': 'cea0af8b-0ebf-4e0e-88d9-69b98c78eefe',\n",
       "  'id': 'eeac7d0a-69e7-4f81-ac5b-35eff542a841',\n",
       "  'modified_at': '2020-07-27T14:55:59.286Z',\n",
       "  'created_at': '2020-07-27T14:55:59.219Z',\n",
       "  'href': '/v4/models/eeac7d0a-69e7-4f81-ac5b-35eff542a841?rev=cea0af8b-0ebf-4e0e-88d9-69b98c78eefe'},\n",
       " 'entity': {'name': 'wml-tensorflow-miregal',\n",
       "  'content_status': {'state': 'persisting'},\n",
       "  'import': {'location': {'training': '3dd32ad4-d6bf-45be-b09f-cf2b79e75e7b',\n",
       "    'pipeline_model': '3dd32ad4-d6bf-45be-b09f-cf2b79e75e7b/pipeline-model.json',\n",
       "    'training_status': '3dd32ad4-d6bf-45be-b09f-cf2b79e75e7b/training-status.json',\n",
       "    'pipeline': '3dd32ad4-d6bf-45be-b09f-cf2b79e75e7b/pipeline.json',\n",
       "    'bucket': 'githubanalyzer-donotdelete-pr-b9xa3kxotzh5in',\n",
       "    'assets_path': '3dd32ad4-d6bf-45be-b09f-cf2b79e75e7b/assets'},\n",
       "   'type': 's3',\n",
       "   'connection': {'access_key_id': 'cc04444c99374c9e9589b8f85e931323',\n",
       "    'secret_access_key': '1a5062d937b09507a05b521a41b8baf6848c0cd6936e2864',\n",
       "    'endpoint_url': 'https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints'}},\n",
       "  'space': {'id': '88740b60-6b2f-4f74-b6d8-20528d14db8b',\n",
       "   'href': '/v4/spaces/88740b60-6b2f-4f74-b6d8-20528d14db8b'},\n",
       "  'type': 'pytorch_1.1',\n",
       "  'runtime': {'id': 'pytorch_1.1-py3.6',\n",
       "   'href': '/v4/runtimes/pytorch_1.1-py3.6'}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"trace\":\"-sulpksdfbwj8\",\"errors\":[{\"code\":\"unsupported_framework_details\",\"message\":\"Models of framework pytorch are not supported.\"}]}\n",
      "\n",
      "\n",
      "--------------------------\n",
      "Deployment creation failed\n",
      "--------------------------\n",
      "\n",
      "\n",
      "2020-07-27T17:00:12.499 WARNING watson_machine_learning_client.wml_client_error.__init__ Deployment creation failed. Error: 400. {\"trace\":\"-sulpksdfbwj8\",\"errors\":[{\"code\":\"unsupported_framework_details\",\"message\":\"Models of framework pytorch are not supported.\"}]}\n"
     ]
    },
    {
     "ename": "WMLClientError",
     "evalue": "Deployment creation failed. Error: 400. {\"trace\":\"-sulpksdfbwj8\",\"errors\":[{\"code\":\"unsupported_framework_details\",\"message\":\"Models of framework pytorch are not supported.\"}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWMLClientError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3e5ef74ce1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigurationMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONLINE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     }\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdeployment_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdeployment_details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/deployments.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, artifact_uid, meta_props, rev_id, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mprint_text_header_h2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mWMLClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Error: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m#return self._handle_response(202, u'created deployment', response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWMLClientError\u001b[0m: Deployment creation failed. Error: 400. {\"trace\":\"-sulpksdfbwj8\",\"errors\":[{\"code\":\"unsupported_framework_details\",\"message\":\"Models of framework pytorch are not supported.\"}]}"
     ]
    }
   ],
   "source": [
    "#\n",
    "# finally let's deploy it\n",
    "#   use model name as deployment name\n",
    "#\n",
    "meta_props = {\n",
    "        client.deployments.ConfigurationMetaNames.NAME: wml_model_name,\n",
    "        client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "deployment_details = client.deployments.create(model_details['metadata']['id'], meta_props)\n",
    "deployment_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-27T16:57:28.619 WARNING watson_machine_learning_client.wml_client_error.__init__ Unexpected type of 'model_uid', expected: '<class 'str'>', actual: '<class 'dict'>'.\n"
     ]
    },
    {
     "ename": "UnexpectedType",
     "evalue": "Unexpected type of 'model_uid', expected: '<class 'str'>', actual: '<class 'dict'>'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedType\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3a037dc416c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/repository.py\u001b[0m in \u001b[0;36mget_model_details\u001b[0;34m(self, model_uid, limit)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \"\"\"\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdocstring_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'str_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTR_TYPE_NAME\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/models.py\u001b[0m in \u001b[0;36mget_details\u001b[0;34m(self, model_uid, limit)\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_either_is_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0mmodel_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_type_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'model_uid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTR_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m         \u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'limit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/watson_machine_learning_client/wml_resource.py\u001b[0m in \u001b[0;36m_validate_type\u001b[0;34m(el, el_name, expected_type, mandatory)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnexpectedType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedType\u001b[0m: Unexpected type of 'model_uid', expected: '<class 'str'>', actual: '<class 'dict'>'."
     ]
    }
   ],
   "source": [
    "model_uid = model_details\n",
    "model_details = client.repository.get_model_details(model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTF ?\n",
    "```\n",
    "Training a PyTorch model using the Watson Machine Learning training service is supported, but deploying a trained PyTorch model in your Watson Machine Learning service is not supported.\n",
    "```\n",
    "https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/pm_service_supported_frameworks.html\n",
    "\n",
    "it took me by surprise: \n",
    "\n",
    "* WML doesn't support training Keras models in the cloud, but you can upload the h5 model and treat it as a tensorflow model\n",
    "\n",
    "Fortunately there is the wml_dev slack channel as last resort ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training parameters\n",
    "\n",
    "```\n",
    "loss_metric: 'mse'    # minimize mean square error\n",
    "optimizer: 'adam'     # sort of adaptive stochastic gradient descent\n",
    "validation_split: 0.2 # 20% of the data is used for validating (val_loss)\n",
    "dropout: 0.3          # ditch 30% of the LSTMs results when minimizing the loss function to avoid overfitting\n",
    "lstm_batch_size: 64   # number of training data batches to evaluate per optimizer run to update the models parameters\n",
    "\n",
    "patience: 10          # try at least 10 times to decrease val_loss smaller by ...\n",
    "min_delta: 0.0003     # ... at least min_delta, else stop, so we get at least 'patience' epochs\n",
    "epochs: 35            # no more than 35 passes through the entier training dataset.\n",
    "\n",
    "l_s: 250              # lookback: num previous timesteps provided to model to predict future values\n",
    "n_predictions: 10     # number of steps ahead to predict\n",
    "```\n",
    "\n",
    "This is defined in `telemanom/config.yaml`\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
